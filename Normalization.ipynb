{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8814e1e8",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdf1fc",
   "metadata": {},
   "source": [
    "The following tokenization function separates punction from words and removes extra whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafe5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28310ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normwix(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[`´‘’ʔ']\", \"'\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"'\", \"ʔ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" +\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[üïɨ+]\", \"ɨ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"ḱ\", \"k\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(ẃ|ẁ)\", \"w\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[ń]\", \"n\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[áàäá]\", \"a\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[éèëéë́]\", \"e\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[íìií]\", \"i\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[óòöó]\", \"o\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[úùú]\", \"u\", text, flags=re.IGNORECASE) \n",
    "    return text\n",
    "\n",
    "#primarily for the bible\n",
    "def aggressive_normwix(text):\n",
    "    text.lower()\n",
    "    text = normwix(text)\n",
    "    text = re.sub(r\"([a-z+])\\1+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ʔ\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"v\", \"w\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(c|qu)\", \"k\", text, flags=re.IGNORECASE)\n",
    "    #text = re.sub(r\"[0-9]+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"ch\", \"ts\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"rr\", \"x\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(?<!t|\\[)s\", \"ts\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"([a-z+])\\1+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ce65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r\"(?<![\\s])([\\)|\\(|.|,|,\\-,\\\"|:|;|¿|?|¡|!])\", r\" \\1\", text)\n",
    "    text = re.sub(r\"([\\)|\\(|.|,|,\\-,\\\"|:|;|¿|?|¡|!])(?<![\\s])\", r\"\\1 \", text)\n",
    "    text = re.sub(r\"(ç|_)\",'',text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\t\",' ',text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"^ \", \"\", text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e63d9",
   "metadata": {},
   "source": [
    "## Detect language to apply appropriate normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490ba28",
   "metadata": {},
   "source": [
    "Since Spanish is present in the Wixárika portion of the data we need to detect it words so that we avoid normalizing Spanish with the orthographic rules of Wixarika.  In order to do this, we will create a synthetic dataset that will contain spanish and wixarika words rerpresented with character count vectors. We will use this dataset to fit a classifer to learn to distinguish between the two languages. Later on, we will use this classifier to apply normalization rules appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a5de1",
   "metadata": {},
   "source": [
    "We begin by separating puncuation from words, removing extra whitespace, and applying a simple normalization shceme to wixárika to remove sporadic diacritics and character repetition. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03744fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(path):\n",
    "    return set(tokenize(open(path,'r').read()).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe85da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_words = get_set('data/es-hch/train.es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77038e23",
   "metadata": {},
   "source": [
    "we apply a simple normalization scheme to remove some of the variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f5edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wixarika_words =  set(normwix(i) for i in get_set('data/es-hch/train.hch'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab623ab",
   "metadata": {},
   "source": [
    "Get the overlap of tokens to use as a proxy for code-switched text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2306616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = wixarika_words.intersection(spanish_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0138f5",
   "metadata": {},
   "source": [
    "Create a vocabulary with both languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c2c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(spanish_words.union(wixarika_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a95597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocab:\n",
    "    if '_' in i:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20964d78",
   "metadata": {},
   "source": [
    "We will use ``vocab`` above as input to a count vectorizer, where each word will be represented as a caracter count vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ca236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5a8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char')\n",
    "vecs = vectorizer.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c81770",
   "metadata": {},
   "source": [
    "Let's take a look at the names of the features, in this cases, the characters that a word could have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f977538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '¿', 'á', 'é', 'í', 'ñ', 'ó', 'ú', 'ü', 'ɨ', 'ʔ', '–']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f443d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_new_word( word):\n",
    "    \n",
    "    return [word.count(i) for i in feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634368c",
   "metadata": {},
   "source": [
    "We now convert `vecs` to an array and create a dictionary that contains a word and its vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09662faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vecs.toarray()\n",
    "word_vecs = {vocab[i]: vectors[i] for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9361de2",
   "metadata": {},
   "source": [
    "We now create a dataset consisting of a word, a label (0 for wixarika, 1 for spanish), and a vector and store them in a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61bb5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073d669",
   "metadata": {},
   "source": [
    "First, we take out the overlap from wixarika words so that they are not labeled as wixarika words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db39dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wix_words = wixarika_words-overlap\n",
    "es_words = spanish_words-overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ed62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[i,1,word_vecs[i]] for i in wix_words] +  [[i,0,word_vecs[i]] for i in es_words]\n",
    "df = pd.DataFrame(data, columns=['word','label','vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "153aaff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tekuni</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nepɨʔena</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tɨyari</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mainɨkɨ</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaniwaye</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17689</th>\n",
       "      <td>apresurado</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17690</th>\n",
       "      <td>soplar</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17691</th>\n",
       "      <td>padres</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17692</th>\n",
       "      <td>confeccionado</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17693</th>\n",
       "      <td>presentaba</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  label                                             vector\n",
       "0             tekuni      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1           nepɨʔena      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2             tɨyari      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3            mainɨkɨ      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4           kaniwaye      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "...              ...    ...                                                ...\n",
       "17689     apresurado      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17690         soplar      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17691         padres      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17692  confeccionado      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17693     presentaba      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[17694 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d3fcb",
   "metadata": {},
   "source": [
    "We now split our data for training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d8299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16595286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in df.vector]\n",
    "y = [i for i in df.label]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16c28b",
   "metadata": {},
   "source": [
    "We now use the data boave to fit a support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0287b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e99bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00aeea4",
   "metadata": {},
   "source": [
    "We now predict the labels for the held-out set and compute its,accuracy, precision,recall, and f1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "908de458",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "374e24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(predictions, y_test)\n",
    "precision = precision_score(predictions, y_test)\n",
    "recall = recall_score(predictions, y_test)\n",
    "f1 = f1_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3468cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.980220401243289\n",
      "Precision : 0.9823807477438762\n",
      "Recall : 0.9874730021598273\n",
      "F1 : 0.984920292977165\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : {}\\nPrecision : {}\\nRecall : {}\\nF1 : {}\".format(accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf31c8b",
   "metadata": {},
   "source": [
    "Let's take a look the labels for the overlapping words Overlapping words classified as spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d409d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_pred = clf.predict([word_vecs[i] for i in overlap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "430ff2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402298850574713"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(cs_pred, [0 for i in range(len(cs_pred))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff7ed31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; 0\n",
      "pasteles 0\n",
      "anillo 0\n",
      "repollos 0\n",
      "tordo 0\n",
      "botes 0\n",
      "017 0\n",
      "jacta 0\n",
      "tras 0\n",
      "antaño 0\n",
      "hollinado 0\n",
      "036 0\n",
      "ti 1\n",
      "serenamente 0\n",
      "juancito 0\n",
      "comadreja 0\n",
      "foresta 0\n",
      "016 0\n",
      "barra 0\n",
      "a 0\n",
      "marfil 0\n",
      "cera 0\n",
      "trizas 0\n",
      "haya 1\n",
      "mismo 0\n",
      "guerreros 0\n",
      "madrina 0\n",
      "046 0\n",
      "molinero 0\n",
      "cesto 0\n",
      "? 0\n",
      "cuatrocientos 0\n",
      "panderetas 0\n",
      "ovillo 0\n",
      "colina 0\n",
      "vagones 0\n",
      "1 0\n",
      "hark 1\n",
      "sendero 0\n",
      "diablillo 0\n",
      "segadores 0\n",
      "poder 0\n",
      "iglesia 0\n",
      "chorlito 0\n",
      "mofarse 0\n",
      "cristal 0\n",
      "guadaña 0\n",
      "cerezas 0\n",
      "mate 1\n",
      "ni 1\n",
      "previos 0\n",
      "paz 0\n",
      "reverencia 0\n",
      "- 0\n",
      "toalla 0\n",
      "segando 0\n",
      "pajas 0\n",
      "primera 0\n",
      "la 0\n",
      "militar 0\n",
      "valla 0\n",
      "castillo 0\n",
      "lima 0\n",
      "peldaño 0\n",
      "bote 0\n",
      "pomerania 0\n",
      "bollo 0\n",
      "arneses 0\n",
      "remos 0\n",
      "establo 0\n",
      "sorbo 0\n",
      "cornetas 0\n",
      "maldijo 0\n",
      "gesto 0\n",
      "afamado 0\n",
      "ya 1\n",
      "mantel 0\n",
      "elsie 0\n",
      "cereales 0\n",
      "he 1\n",
      "006 0\n",
      "perspicaz 0\n",
      "cenicienta 0\n",
      "3 0\n",
      "molinera 0\n",
      "hechicera 0\n",
      "tirita 1\n",
      "sirope 0\n",
      "012 0\n",
      "granuja 0\n",
      "escuela 0\n",
      "padre 0\n",
      "como 0\n",
      "novia 0\n",
      "mirlo 0\n",
      "hora 0\n",
      "perezosa 0\n",
      "media 0\n",
      "cerradura 0\n",
      "justicia 0\n",
      "margarita 0\n",
      "cegado 0\n",
      "rey 1\n",
      "014 0\n",
      "hans 0\n",
      "devoraba 0\n",
      "fiel 0\n",
      "031 0\n",
      "splash 0\n",
      "san 0\n",
      "bremen 0\n",
      "reyes 0\n",
      "semsi 0\n",
      "suerte 0\n",
      "imperial 0\n",
      "lobo 0\n",
      "trono 0\n",
      "matrimonio 0\n",
      "granos 0\n",
      "( 0\n",
      "peras 0\n",
      "amuleto 0\n",
      "sesenta 0\n",
      "lino 0\n",
      "mozo 0\n",
      "valles 0\n",
      "jorobado 0\n",
      "para 0\n",
      "acongojado 0\n",
      "rapunzel 0\n",
      "guerrero 0\n",
      "piso 0\n",
      "llave 0\n",
      "pulgarcito 0\n",
      "018 0\n",
      "zachiel 0\n",
      "te 1\n",
      "croak 1\n",
      "barril 0\n",
      "gorra 0\n",
      "monstruo 0\n",
      "hierro 0\n",
      "moraban 0\n",
      "gorro 0\n",
      "modales 0\n",
      "009 0\n",
      "posadero 0\n",
      "emperadores 0\n",
      "pastor 0\n",
      "boda 0\n",
      "mermelada 0\n",
      "mana 1\n",
      "gorriones 0\n",
      "033 0\n",
      "avellanas 0\n",
      "papas 0\n",
      "patito 0\n",
      "trompetas 0\n",
      "oro 0\n",
      "orgullo 0\n",
      "048 0\n",
      "024 0\n",
      "mercado 0\n",
      "primaveral 0\n",
      "riachuelos 0\n",
      "astuto 0\n",
      "pena 1\n",
      "011 0\n",
      "brandy 0\n",
      "garras 0\n",
      "alianza 0\n",
      "joyas 0\n",
      "038 0\n",
      "molino 0\n",
      "super 0\n",
      "erizo 0\n",
      "042 0\n",
      "colinas 0\n",
      "postres 0\n",
      "excepto 0\n",
      "calle 0\n",
      "corso 0\n",
      "fermento 0\n",
      "regocijo 0\n",
      "buhardilla 0\n",
      "holgazanes 0\n",
      "uso 0\n",
      "allerleirauh 0\n",
      "hola 0\n",
      "008 0\n",
      "mantelito 0\n",
      "gradas 0\n",
      "sino 0\n",
      "guantes 0\n",
      "035 0\n",
      "diablo 0\n",
      "roar 0\n",
      "testimonio 0\n",
      "enfermera 0\n",
      "verdugo 0\n",
      "reino 0\n",
      "estanque 0\n",
      "comestible 0\n",
      "paja 0\n",
      "pero 0\n",
      "gaspar 0\n",
      "tenedor 0\n",
      "mata 1\n",
      "007 0\n",
      "cinta 0\n",
      "baño 0\n",
      "bellos 0\n",
      "don 0\n",
      "panqueques 0\n",
      "corona 0\n",
      "037 0\n",
      "zapatillas 0\n",
      "ji 0\n",
      "cerraduras 0\n",
      "diamante 0\n",
      "real 0\n",
      "astilla 0\n",
      "c 0\n",
      "cebada 0\n",
      "duques 0\n",
      "pajarito 0\n",
      "reina 1\n",
      "bruja 0\n",
      "alteza 0\n",
      "029 0\n",
      "manzano 0\n",
      "pueblos 0\n",
      "enigma 0\n",
      "sortija 0\n",
      "mijo 0\n",
      "baltazar 0\n",
      "cuu 0\n",
      "diamantes 0\n",
      "023 0\n",
      "nombres 0\n",
      "unicornio 0\n",
      "varita 0\n",
      "tesoros 0\n",
      "sirvienta 0\n",
      "pastores 0\n",
      "doncella 0\n",
      "028 0\n",
      "pimientos 0\n",
      "039 0\n",
      "trigo 0\n",
      "pic 0\n",
      "vinagre 0\n",
      "acongojada 0\n",
      "020 0\n",
      "emperador 0\n",
      "palomita 0\n",
      "susurro 0\n",
      "021 0\n",
      "espesura 0\n",
      "roe 0\n",
      "pichel 0\n",
      "moras 0\n",
      "cestas 0\n",
      "brujo 0\n",
      "sello 0\n",
      "de 0\n",
      "bollos 0\n",
      "zapatero 0\n",
      "avellanos 0\n",
      "huso 0\n",
      "me 1\n",
      "tesoro 0\n",
      "angelito 0\n",
      "avellano 0\n",
      "sinceramente 0\n",
      "ogro 0\n",
      "tujoo 0\n",
      "percance 0\n",
      "yoringel 0\n",
      "esponja 0\n",
      "bayas 0\n",
      "perlas 0\n",
      "cielos 0\n",
      "blanca 0\n",
      ". 0\n",
      "setenta 1\n",
      "puerta 1\n",
      "caballo 0\n",
      "no 0\n",
      "muchedumbre 0\n",
      "brisa 0\n",
      "ingenuo 0\n",
      "promesa 0\n",
      "musgo 0\n",
      "lamento 0\n",
      "tareas 0\n",
      "emperadora 0\n",
      "jorge 0\n",
      "blanquita 0\n",
      "botero 0\n",
      "corcel 0\n",
      "terciopelo 0\n",
      "pastel 0\n",
      "obispo 0\n",
      "013 0\n",
      "azote 0\n",
      "019 0\n",
      "caverna 0\n",
      "mes 0\n",
      "b 0\n",
      "pellejo 0\n",
      "sedas 0\n",
      "rufianes 0\n",
      "isabel 0\n",
      "esmeraldas 0\n",
      "044 0\n",
      "modesta 0\n",
      "pato 0\n",
      "entonces 0\n",
      "sirviente 0\n",
      "nueces 0\n",
      "ducado 0\n",
      "jaula 0\n",
      "seda 0\n",
      "hey 1\n",
      "fijo 0\n",
      "majestad 0\n",
      "villa 0\n",
      "maravillas 0\n",
      ": 0\n",
      "yorinda 0\n",
      "sastre 0\n",
      "tocino 0\n",
      "abogado 0\n",
      "pared 0\n",
      "paloma 0\n",
      "tallos 0\n",
      "perno 0\n",
      "princesa 0\n",
      "posada 0\n",
      "nabo 0\n",
      "aja 0\n",
      "lentejas 0\n",
      "032 0\n",
      "pasmado 0\n",
      "¡ 0\n",
      "010 0\n",
      "afeitado 0\n",
      "capturarla 0\n",
      "pura 1\n",
      "zapatilla 0\n",
      "oh 0\n",
      "alcalde 0\n",
      "rana 0\n",
      "026 0\n",
      "nombre 0\n",
      "conrad 0\n",
      "mendigo 0\n",
      "palacio 0\n",
      "ventana 0\n",
      "banda 0\n",
      "henry 1\n",
      "plata 0\n",
      "¿ 0\n",
      "jardines 0\n",
      "hechiceras 0\n",
      "korbes 0\n",
      "va 0\n",
      "avena 0\n",
      ", 0\n",
      "cerveza 0\n",
      "princesas 0\n",
      "rendija 0\n",
      "hansel 0\n",
      "encantador 0\n",
      "lobato 0\n",
      "045 0\n",
      "mago 0\n",
      "pizca 0\n",
      "finca 0\n",
      "arrogancia 0\n",
      "tarea 0\n",
      "nieves 0\n",
      "cesta 0\n",
      "hasta 0\n",
      "perdices 0\n",
      "030 0\n",
      "marinero 0\n",
      "nopi 0\n",
      "carbunclos 0\n",
      "caballeros 0\n",
      "sastrecillo 0\n",
      "esplendor 0\n",
      "del 0\n",
      "nabos 0\n",
      "005 0\n",
      "fantasma 0\n",
      "hay 1\n",
      "jaulas 0\n",
      "cama 0\n",
      "ha 1\n",
      "mi 1\n",
      "regla 0\n",
      "tormento 0\n",
      "pulgar 0\n",
      "barones 0\n",
      "bala 0\n",
      "\" 0\n",
      "arre 0\n",
      "trueque 0\n",
      "deseos 0\n",
      "cien 0\n",
      "o 0\n",
      "perla 0\n",
      "cereal 0\n",
      "señor 0\n",
      "! 0\n",
      "avaro 0\n",
      "manzanas 0\n",
      "polluelos 0\n",
      "el 0\n",
      "manzana 0\n",
      "y 1\n",
      "rapunzeles 0\n",
      "recolecta 0\n",
      "vivaces 0\n",
      "hecatombe 0\n",
      "grethel 0\n",
      "melchor 0\n",
      "potro 0\n",
      ") 0\n",
      "amo 0\n",
      "cetro 0\n",
      "tujii 1\n",
      "maña 0\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(cs_pred, overlap):\n",
    "    print(j,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32527806",
   "metadata": {},
   "source": [
    "# Reconstructing  word initial glottal stops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd65a69",
   "metadata": {},
   "source": [
    "Now that we have trained our classifier to detect the language of a word, we can use it to reconstruct the word initial glottal stop in wixarika words only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fd7c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_glottal(sentences,clf=clf, aggressive=False):\n",
    "    sents= []\n",
    "    norming = normwix\n",
    "    if aggressive:\n",
    "        norming = aggressive_normwix\n",
    "    for j in sentences:\n",
    "        \n",
    "        sent = tokenize(norming(j)).split()\n",
    "        sent_labels = [vectorize_new_word(word) for word in sent]\n",
    "        pr = clf.predict(sent_labels)\n",
    "        normed_sent = [\"ʔ\"+i if (j==1 and i[0] in 'ɨaeiu') else i for i,j in zip(sent,pr)]\n",
    "        sents.append(\" \".join(normed_sent))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c11442",
   "metadata": {},
   "source": [
    "# Norming labialization of velar stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f48b46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_labialization(sentence):\n",
    "    sentence = re.sub('kw','ku', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0164effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renorm(input_path,output,aggressive=False):\n",
    "    normalized =  reconstruct_glottal(open(input_path,'r').readlines(),aggressive=aggressive)\n",
    "    \n",
    "    with open(output,'w') as f:\n",
    "        f.write(collapse_labialization(\"\\n\".join(normalized)))\n",
    "    print(f\"{input_path} renormed and written to {output}\" )\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc471f25",
   "metadata": {},
   "source": [
    "# The Spanish-Wixarika Parallel Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc385b",
   "metadata": {},
   "source": [
    "Write normalized data for NMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c912c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/es-hch/train.hch renormed and written to clean/es-hch/train.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='data/es-hch/train.hch',\n",
    "       output='clean/es-hch/train.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e09de1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/es-hch/dev.hch renormed and written to clean/es-hch/dev.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='data/es-hch/dev.hch',\n",
    "       output='clean/es-hch/dev.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc72d12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/es-hch/test.hch renormed and written to clean/es-hch/test.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='data/es-hch/test.hch',\n",
    "       output='clean/es-hch/test.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a8543da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train','dev','test']:\n",
    "    ss = f\"es-hch/{t}.es\"\n",
    "    sen = open(f'data/{ss}','r').readlines()\n",
    "    with open(f'clean/{ss}','w') as f:\n",
    "        f.write(\"\".join([tokenize(i).lower() for i in sen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba0c0c",
   "metadata": {},
   "source": [
    "# Descriptive Grammars Gomez (1998) and Ramos-Bierge(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bba12e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch/Gomez/gomez.hch renormed and written to clean/Gomez/gomez.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='scratch/Gomez/gomez.hch',\n",
    "       output='clean/Gomez/gomez.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca3b9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch/Ramos/ramos.hch renormed and written to clean/Ramos/ramos.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='scratch/Ramos/ramos.hch',\n",
    "       output='clean/Ramos/ramos.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb5009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['Ramos','Gomez']:\n",
    "    ss = f\"{t}/{t.lower()}.es\"\n",
    "    sen = open(f'data/{ss}','r').readlines()\n",
    "    with open(f'clean/{ss}','w') as f:\n",
    "        f.write(\"\".join([tokenize(i).lower() for i in sen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107814a",
   "metadata": {},
   "source": [
    "# AmericasNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e86d5e",
   "metadata": {},
   "source": [
    "[AmericasNLI](https://arxiv.org/pdf/2104.08726.pdf) is an extension of [XNLI](https://arxiv.org/pdf/1809.05053.pdf) to 10 Indigenous languages of the Americas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98d10694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch/AmericasNLI/dev.hch renormed and written to clean/AmericasNLI/dev.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='scratch/AmericasNLI/dev.hch',\n",
    "       output='clean/AmericasNLI/dev.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8759c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch/AmericasNLI/test.hch renormed and written to clean/AmericasNLI/americasnli.hch\n"
     ]
    }
   ],
   "source": [
    "renorm(input_path='scratch/AmericasNLI/test.hch',\n",
    "       output='clean/AmericasNLI/americasnli.hch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4293167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['test']:\n",
    "    ss = f\"AmericasNLI/americasnli.es\"\n",
    "    sen = open(f'data/{ss}','r').readlines()\n",
    "    with open(f'clean/{ss}','w') as f:\n",
    "        f.write(\"\".join([tokenize(i).lower() for i in sen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60659e",
   "metadata": {},
   "source": [
    "# Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976eb63",
   "metadata": {},
   "source": [
    "Most local languages around the world lack the resources to build robust machine translation systems. The bible may be a useful resource since it has been translated to more than 3000 languages. In two to three sentences describe some of the ethical implications of using the bible to train a machine translation system for a local language? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5f5700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### your answer goes here. ####\n",
    "answer = \"\"\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "753b3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_bible = open('scratch/Bible/spa-x-bible-hablahoi-latina.txt.jhubc','r').readlines()\n",
    "wixarika_bible = open('scratch/Bible/hch-x-bible-hch-v1.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95c385a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = [(i,j) for i,j in zip(wixarika_bible,spanish_bible) if len(i) > 1 and len(j) > 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb7fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94da1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df = pd.DataFrame(bible,columns=['wixarika','spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04fc9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               wixarika  \\\n",
      "5333  Me xüca peneüqueni , 'aixüa cani'aneni , 'axa ...   \n",
      "\n",
      "                                                spanish  \n",
      "5333  Si te casas , no cometes pecado ; y si una muj...  \n",
      "                                               wixarika  \\\n",
      "5334  'Ipaü nepaine ne'ivama , tucari canaye'aximeni...   \n",
      "\n",
      "                                                spanish  \n",
      "5334  Hermanos , lo que quiero decir es esto : Nos q...  \n",
      "                                               wixarika  \\\n",
      "5335  mümeta memutisuana müme memüca'utisuana vahepa...   \n",
      "\n",
      "                                                spanish  \n",
      "5335  los que están de luto deben portarse como si e...  \n"
     ]
    }
   ],
   "source": [
    "for i in range(bible_df.shape[0]):\n",
    "    if \"que'ane\" in bible_df.wixarika[i]:\n",
    "        print((bible_df[i-1:i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79f61c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    'Inüari canihücütüni 'icü . Nuivarite mecani'i...\n",
       "1    'Apurahami 'Isahaqui cani'uquiyarieyatücaitüni...\n",
       "2    Cura meta Parexi Sara püva'uquiyaritücai , Tam...\n",
       "3    'Arami 'Aminaravi pü'uquiyarieyatücai . 'Amina...\n",
       "4    Sarumuni Puhuxi pü'uquiyarieyatücai , Xahavi v...\n",
       "5    Quisahi Raviri pü'uquiyarieyatücai , que mü'an...\n",
       "6    Sarumuni Xupuhami pü'uquiyarieyatücai . Xupuha...\n",
       "7    'Asa Cusapati pü'uquiyarieyatücai . Cusapati C...\n",
       "8    'Usiyaxi Cutami pü'uquiyarieyatücai . Cutami '...\n",
       "9    'Esequiyaxi Manasexi pü'uquiyarieyatücai . Man...\n",
       "Name: wixarika, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df[:10].wixarika"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9794d",
   "metadata": {},
   "source": [
    "We select 1000 random sentences for tesing and roughly 10% for developemnt, note that we will only use the test set. Training NMT with the Bible is left for future work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "395f8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "hch_train, hch_eval, es_train, es_eval = train_test_split( bible_df.wixarika, bible_df.spanish, test_size=0.2260,  random_state=42)\n",
    "\n",
    "hch_dev, hch_test, es_dev, es_test = train_test_split( hch_eval, es_eval, test_size=0.559,  random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ee5cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_splits = {'train' : {'es':es_train,'hch':hch_train},\n",
    "'dev' : {'es':es_dev,'hch':hch_dev },\n",
    "'test' : {'es':es_test,'hch':hch_test}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "597a38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train','dev','test']:\n",
    "    for lang in ['es','hch']:\n",
    "        with open(f'scratch/Bible/{split}.{lang}','w') as f:\n",
    "            f.write(\"\".join(bible_splits[split][lang]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3fb558",
   "metadata": {},
   "source": [
    "Wixárika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "862dac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch/Bible/test.hch renormed and written to clean/Bible/bible.hch\n"
     ]
    }
   ],
   "source": [
    "suffix='hch'\n",
    "for split in ['test']:\n",
    "    renorm(input_path=f'scratch/Bible/{split}.{suffix}', \n",
    "           output=f'clean/Bible/bible.{suffix}',aggressive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab0925",
   "metadata": {},
   "source": [
    "Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e92f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'es'\n",
    "for t in ['test']:\n",
    "    ss = f\"Bible/bible.{suffix}\"\n",
    "    sen = open(f'scratch/{ss}','r').readlines()\n",
    "    with open(f'clean/{ss}','w') as f:\n",
    "        f.write(\"\".join([tokenize(i).lower() for i in sen]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4d4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
