{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d077e496",
   "metadata": {},
   "source": [
    "# Evaluating Morphological Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a6d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import morfessor as morf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0688878",
   "metadata": {},
   "source": [
    "In this notebook, we will evaluate different tokenizers on their ability to correctly parse morphological boundaries for wixárika and spanish. In order to do this, we will implement a variety of tokenization methods. Then we will evalute them on a 4 datasets, 2 in Spanish and 2 in Wixárika. The metrics that we will use are the Border F1 and associated Precision and Recall scores. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217ace4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa9683",
   "metadata": {},
   "source": [
    "## Gold Morphological Segmentations\n",
    "The datasets we will use contain words annotated with morphological boundaries. They come from two grammars () and ().\n",
    "Note that the words in the datasests are capitalized and the tokenizers were trained to only handle lowercased data. For the wixarika tokenizers, the choice in unicode characters is also really important. Make sure that you use appropriate characters when testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "316e5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c388038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normwix(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[`´‘’ʔ']\", \"'\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"'\", \"ʔ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" +\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[üïɨ+]\", \"ɨ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"ḱ\", \"k\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(ẃ|ẁ)\", \"w\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[ń]\", \"n\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[áàäá]\", \"a\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[éèëéë́]\", \"e\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[íìií]\", \"i\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[óòöó]\", \"o\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[úùú]\", \"u\", text, flags=re.IGNORECASE)\n",
    "     \n",
    "    return text\n",
    "\n",
    "#primarily for the bible\n",
    "def aggressive_normwix(text):\n",
    "    text.lower()\n",
    "    text = normwix(text)\n",
    "    text = re.sub(r\"([a-z+])\\1+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ʔ\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"v\", \"w\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(c|qu)\", \"k\", text, flags=re.IGNORECASE)\n",
    "    #text = re.sub(r\"[0-9]+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"ch\", \"ts\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"rr\", \"x\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(?<!t|\\[)s\", \"ts\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"([a-z+])\\1+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"(?<![\\s])([\\)|\\(|.|,|,\\-,\\\"|:|;|¿|?|¡|!])\", r\" \\1\", text)\n",
    "    text = re.sub(r\"([\\)|\\(|.|,|,\\-,\\\"|:|;|¿|?|¡|!])(?<![\\s])\", r\"\\1 \", text)\n",
    "    text = re.sub(r\"(ç|_)\",'',text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\t\",' ',text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"^ \", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f8a51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x):\n",
    "    ### your code goes here ###\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25a0d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(path,normalization=normalization):\n",
    "    df = pd.read_csv(path)\n",
    "    df['word'] = df.segmentations.apply(lambda x: unmorph(x))\n",
    "    df['normalized'] = df.word.apply(lambda x: normalization(x))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a97a788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentations</th>\n",
       "      <th>word</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A*</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A*c|o|s|t|u|m|b|r*o*</td>\n",
       "      <td>Acostumbro</td>\n",
       "      <td>Acostumbro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A*g|a|r|r*e*n*</td>\n",
       "      <td>Agarren</td>\n",
       "      <td>Agarren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A*l*</td>\n",
       "      <td>Al</td>\n",
       "      <td>Al</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A|c|u|é|r|d*a*t|e*</td>\n",
       "      <td>Acuérdate</td>\n",
       "      <td>Acuérdate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          segmentations        word  normalized\n",
       "0                    A*           A           A\n",
       "1  A*c|o|s|t|u|m|b|r*o*  Acostumbro  Acostumbro\n",
       "2        A*g|a|r|r*e*n*     Agarren     Agarren\n",
       "3                  A*l*          Al          Al\n",
       "4    A|c|u|é|r|d*a*t|e*   Acuérdate   Acuérdate"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_gomez = dataset('gold/spanish.gomez')\n",
    "spanish_gomez.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5b3040a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentations</th>\n",
       "      <th>word</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'|+|k|w|a|i*</td>\n",
       "      <td>'+kwai</td>\n",
       "      <td>ʔɨkwai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'|+|p|a|r|i*t|s|i|e*</td>\n",
       "      <td>'+paritsie</td>\n",
       "      <td>ʔɨparitsie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'|+|x|a*</td>\n",
       "      <td>'+xa</td>\n",
       "      <td>ʔɨxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'|a*'|i|w|a*m|a*</td>\n",
       "      <td>'a'iwama</td>\n",
       "      <td>ʔaʔiwama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'|a*k|a|w|a|y|u*</td>\n",
       "      <td>'akawayu</td>\n",
       "      <td>ʔakawayu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          segmentations        word  normalized\n",
       "0          '|+|k|w|a|i*      '+kwai      ʔɨkwai\n",
       "1  '|+|p|a|r|i*t|s|i|e*  '+paritsie  ʔɨparitsie\n",
       "2              '|+|x|a*        '+xa        ʔɨxa\n",
       "3      '|a*'|i|w|a*m|a*    'a'iwama    ʔaʔiwama\n",
       "4      '|a*k|a|w|a|y|u*    'akawayu    ʔakawayu"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wixarika_gomez = dataset('gold/wixarika.gomez',aggressive_normwix)\n",
    "wixarika_gomez.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7682bb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentations</th>\n",
       "      <th>word</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A|l|l*í*</td>\n",
       "      <td>Allí</td>\n",
       "      <td>Allí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A|n|d*a*b|a*n*</td>\n",
       "      <td>Andaban</td>\n",
       "      <td>Andaban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E*l*</td>\n",
       "      <td>El</td>\n",
       "      <td>El</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E|s|t*e*</td>\n",
       "      <td>Este</td>\n",
       "      <td>Este</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E|s|t*o*</td>\n",
       "      <td>Esto</td>\n",
       "      <td>Esto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    segmentations     word normalized\n",
       "0        A|l|l*í*     Allí       Allí\n",
       "1  A|n|d*a*b|a*n*  Andaban    Andaban\n",
       "2            E*l*       El         El\n",
       "3        E|s|t*e*     Este       Este\n",
       "4        E|s|t*o*     Esto       Esto"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_ramos  = dataset('gold/spanish.ramos')\n",
    "spanish_ramos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1796eaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segmentations</th>\n",
       "      <th>word</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'|+|i|m|a|r|i*</td>\n",
       "      <td>'+imari</td>\n",
       "      <td>ʔɨimari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'|+|k|+*</td>\n",
       "      <td>'+k+</td>\n",
       "      <td>ʔɨkɨ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'|+|k|i|t|s|i|k|a*</td>\n",
       "      <td>'+kitsika</td>\n",
       "      <td>ʔɨkitsika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'|+|p|a|r|i*t|s|i|e*</td>\n",
       "      <td>'+paritsie</td>\n",
       "      <td>ʔɨparitsie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'|+|p|i|n|a*</td>\n",
       "      <td>'+pina</td>\n",
       "      <td>ʔɨpina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          segmentations        word  normalized\n",
       "0        '|+|i|m|a|r|i*     '+imari     ʔɨimari\n",
       "1              '|+|k|+*        '+k+        ʔɨkɨ\n",
       "2    '|+|k|i|t|s|i|k|a*   '+kitsika   ʔɨkitsika\n",
       "3  '|+|p|a|r|i*t|s|i|e*  '+paritsie  ʔɨparitsie\n",
       "4          '|+|p|i|n|a*      '+pina      ʔɨpina"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wixarika_gramos  = dataset('gold/wixarika.ramos',aggressive_normwix)\n",
    "wixarika_gramos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef07d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns character boundaries as a vector with \n",
    "# 1 representing morpheme boundary, 0 otherwise\n",
    "\n",
    "def as_ones(word):\n",
    "    ones = []\n",
    "    for i in word:\n",
    "        if i == '*':\n",
    "            ones.append(1)\n",
    "        if i == '|':\n",
    "            ones.append(0)\n",
    "    return ones\n",
    "\n",
    "# returns a word without character or morph\n",
    "# boundaries\n",
    "def unmorph(text):\n",
    "    text = re.sub(r\"[\\|\\*]\", '', text)\n",
    "    return text\n",
    "\n",
    "# returns a word with morpheme boundaries\n",
    "# as spaces, undoes morphprepare\n",
    "def space_morph(text):\n",
    "    text = re.sub(r\"[\\|]\", '', text)\n",
    "    text = re.sub(r\"[\\*]\", ' ', text)\n",
    "    return text\n",
    "\n",
    "#returns the word form\n",
    "def word_form(text):\n",
    "    return re.sub(r\" \",\"\", space_morph(text))\n",
    "    \n",
    "def metrics(y, y_hat):\n",
    "    assert len(y) == len(y_hat), (y,y_hat)\n",
    "    ratio = y_hat.count('*')/y.count('*')\n",
    "    tr,pr =  np.array([(i,j) for i,j in zip(y,y_hat) if \"*\" in (i,j)]).T\n",
    "    tr,pr = as_ones(tr), as_ones(pr)\n",
    "    precision = precision_score(tr,pr, zero_division = 1)\n",
    "    recall = recall_score(tr,pr, zero_division = 1)\n",
    "    f1 =  f1_score(tr,pr, zero_division = 1)\n",
    "    token_accuracy = int(y==y_hat)\n",
    "    return( precision, recall, f1, token_accuracy, ratio)\n",
    "\n",
    "def morph_metrics(gold,segmentations):  \n",
    "    prec, reca,f1,acc,ra = [], [], [], [], []\n",
    "    for y,y_hat in zip(gold, segmentations):     \n",
    "        a = metrics(y, y_hat)\n",
    "        prec.append(a[0])\n",
    "        reca.append(a[1])\n",
    "        f1.append(a[2])\n",
    "        acc.append(a[3])\n",
    "        ra.append(a[4])\n",
    "    m = {'precision': prec , 'recall': reca, 'f1': f1,'token_accuracy':acc, 'ratio':ra}\n",
    "    return {i: np.array(m[i]).mean() for i in m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"u|n*a*s*\")\n",
    "[0,1,1,1], "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39f839",
   "metadata": {},
   "source": [
    "## Word level tokenization\n",
    "\n",
    "In word level tokenization, we separate words by spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c9f9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = spanish_gomez.segmentations.apply(lambda x : ''.join([i+'|' for i in unmorph(x.lower())])[:-1]+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25510d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d729eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [(i,j) for i,j in (set([i for  i in zip(spanish_gomez.segmentations.apply(lambda x: x.lower()), word)]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21543000",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_segementations = pd.DataFrame(w, columns=['gold','prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f1cc25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0,\n",
       " 'recall': 0.5234897870491091,\n",
       " 'f1': 0.6620599739243808,\n",
       " 'token_accuracy': 0.16297262059973924,\n",
       " 'ratio': 0.5234897870491091}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_metrics(word_segementations.gold, word_segementations.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4813e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c35e4",
   "metadata": {},
   "source": [
    "## Character level tokenization\n",
    "In character level tokenization, we separte words by characters and use a special character to denote word boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ead26e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char = spanish_gomez.segmentations.apply(lambda x : ''.join([i+'*' for i in unmorph(x)]))\n",
    "c = [(i,j) for i,j in (set([i for  i in zip(spanish_gomez.segmentations.apply(lambda x: x.lower()), char)]))]\n",
    "char_segementations = pd.DataFrame(c, columns=['gold','prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29be8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.4403130277692194,\n",
       " 'recall': 1.0,\n",
       " 'f1': 0.5901873596767226,\n",
       " 'token_accuracy': 0.036214953271028034,\n",
       " 'ratio': 2.7003504672897196}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_metrics(char_segementations.gold, char_segementations.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ab75e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db547e",
   "metadata": {},
   "source": [
    "# Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e585d7",
   "metadata": {},
   "source": [
    "In this task you will be training multiple tokenization methods by modifying the input and method-specific hyperparamters. \n",
    "\n",
    "\n",
    "\n",
    "1. Unnormalized: the data as is\n",
    "2. Punctuation normalization : separating punction from words using white space\n",
    "3. Aggressive normalization: punctuation normalization + langugage specific normalization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d737688",
   "metadata": {},
   "source": [
    "## Morfessor\n",
    "\n",
    "Morfessor is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ff16d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UnsupervisedSegmenters import MorfessorTokenizer as morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b00764f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a3328aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_morf = morfessor()\n",
    "spanish_morf.load_model('tokenizers/morf/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4767bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "morfs =  spanish_gomez.segmentations.apply(lambda x: \n",
    "                                         \"|\".join([ i for i in \"*\".join(spanish_morf.segment_word(unmorph(x).lower(), n=1))+'*']\n",
    "                                         ).replace('|*|','*').replace('|*','*'))\n",
    "\n",
    "m = [(i,j) for i,j in (set([i for  i in zip(spanish_gomez.segmentations.apply(lambda x: x.lower()), morfs)]))]\n",
    "morf_segementations = pd.DataFrame(m, columns=['gold','prediction'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c552ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7204476314645807,\n",
       " 'recall': 0.6460669274228595,\n",
       " 'f1': 0.6476324993274146,\n",
       " 'token_accuracy': 0.16297262059973924,\n",
       " 'ratio': 1.0280312907431552}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_metrics(morf_segementations.gold, morf_segementations.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3b1dd3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g|u|s|t*a*</td>\n",
       "      <td>g|u|s|t|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m|u|n|i|c|i|p*a|l*</td>\n",
       "      <td>m|u*n|i*c|i*p|a*l*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h|i|n|c|h*ó*</td>\n",
       "      <td>h|i*n*c|h*ó*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h|a|b|r*í*a*</td>\n",
       "      <td>h|a|b*r|í|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v|a|r|i*a*s*</td>\n",
       "      <td>v|a|r|i|a|s*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p|i|d*o*</td>\n",
       "      <td>p|i|d|o*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f|l|a|c*o*</td>\n",
       "      <td>f|l|a|c|o*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m*í*a*</td>\n",
       "      <td>m|í*a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s|i|g*a*</td>\n",
       "      <td>s|i*g|a*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 gold          prediction\n",
       "0          g|u|s|t*a*          g|u|s|t|a*\n",
       "1  m|u|n|i|c|i|p*a|l*  m|u*n|i*c|i*p|a*l*\n",
       "2        h|i|n|c|h*ó*        h|i*n*c|h*ó*\n",
       "3        h|a|b|r*í*a*        h|a|b*r|í|a*\n",
       "4        v|a|r|i*a*s*        v|a|r|i|a|s*\n",
       "5            p|i|d*o*            p|i|d|o*\n",
       "6          n|o|c|h|e*          n|o|c|h|e*\n",
       "7          f|l|a|c*o*          f|l|a|c|o*\n",
       "8              m*í*a*              m|í*a*\n",
       "9            s|i|g*a*            s|i*g|a*"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morf_segementations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13acb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f32c7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencepiece_encode(x,model):\n",
    "    x = unmorph(x)\n",
    "    x = \" \".join(model.encode(x.lower(), out_type=str)).replace('▁ ','').replace('▁','')\n",
    "    x = x.replace(' ','*')+'*'\n",
    "    x = \"|\".join([i for i in x]).replace('|*|','*').replace('|*','*')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db71e6",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03f608",
   "metadata": {},
   "source": [
    "Train byte-pair encoding tokenizers using the following configurations:\n",
    "\n",
    "1. Normalization\n",
    "    1. raw\n",
    "    2. punctuation normalization \n",
    "    3. aggresive normalization\n",
    "\n",
    "\n",
    "2. vocabulary size\n",
    "    1. 500 \n",
    "    2. 1000 \n",
    "    3. 2000\n",
    "    4. 3000\n",
    "    5. 4000\n",
    "    6. 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4aab61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ecd4c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_bpe = spm.SentencePieceProcessor(model_file='tokenizers/bpe/spanish.5000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6fac21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe  = spanish_gomez.segmentations.apply(lambda x: sentencepiece_encode(x,model=spanish_bpe))                                         \n",
    "b = [(i,j) for i,j in (set([i for  i in zip(spanish_gomez.segmentations.apply(lambda x: x.lower()), bpe)]))]\n",
    "bpe_segementations = pd.DataFrame(b, columns=['gold','prediction'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a23c3022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8399174272055628,\n",
       " 'recall': 0.5809865275966971,\n",
       " 'f1': 0.649088388071439,\n",
       " 'token_accuracy': 0.1694915254237288,\n",
       " 'ratio': 0.7872229465449804}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_metrics(bpe_segementations.gold, bpe_segementations.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b8b7163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n|i|ñ*o*s*</td>\n",
       "      <td>n|i|ñ|o|s*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g|u|s|t*a*</td>\n",
       "      <td>g|u|s|t|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e|n*f|r|e|n|t*e*</td>\n",
       "      <td>e|n|f*r|e*n|t|e*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p|i|d*o*</td>\n",
       "      <td>p|i|d|o*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c|r|e*e*s*</td>\n",
       "      <td>c|r|e|e|s*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f|l|a|c*o*</td>\n",
       "      <td>f|l|a|c|o*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s|i|g*a*</td>\n",
       "      <td>s|i*g|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p|r|i|s|a*</td>\n",
       "      <td>p|r|i|s|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a|r|r|i|b*a*</td>\n",
       "      <td>a|r|r|i|b|a*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               gold        prediction\n",
       "0        n|i|ñ*o*s*        n|i|ñ|o|s*\n",
       "1        g|u|s|t*a*        g|u|s|t|a*\n",
       "2  e|n*f|r|e|n|t*e*  e|n|f*r|e*n|t|e*\n",
       "3          p|i|d*o*          p|i|d|o*\n",
       "4        n|o|c|h|e*        n|o|c|h|e*\n",
       "5        c|r|e*e*s*        c|r|e|e|s*\n",
       "6        f|l|a|c*o*        f|l|a|c|o*\n",
       "7          s|i|g*a*          s|i*g|a*\n",
       "8        p|r|i|s|a*        p|r|i|s|a*\n",
       "9      a|r|r|i|b*a*      a|r|r|i|b|a*"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_segementations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19d5d",
   "metadata": {},
   "source": [
    "## Unigram Language Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994228a",
   "metadata": {},
   "source": [
    "Train unigram language modeling tokenizers using the following configurations:\n",
    "\n",
    "1. Normalization\n",
    "    1. raw\n",
    "    2. punctuation normalization \n",
    "    3. aggresive normalization\n",
    "\n",
    "\n",
    "2. vocabulary size\n",
    "    1. 500 \n",
    "    2. 1000 \n",
    "    3. 2000\n",
    "    4. 3000\n",
    "    5. 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e08d945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_ulm = spm.SentencePieceProcessor(model_file='tokenizers/unigram/spanish.4000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d4f4a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "94f7b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulm  = spanish_gomez.segmentations.apply(lambda x: sentencepiece_encode(x,model=spanish_ulm))                                         \n",
    "u = [(i,j) for i,j in (set([i for  i in zip(spanish_gomez.segmentations.apply(lambda x: x.lower()), ulm)]))]\n",
    "ulm_segementations = pd.DataFrame(u, columns=['gold','prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "512e10ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8585614950021729,\n",
       " 'recall': 0.6725771403737506,\n",
       " 'f1': 0.715507750253513,\n",
       " 'token_accuracy': 0.2242503259452412,\n",
       " 'ratio': 0.8934159061277706}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_metrics(ulm_segementations.gold, ulm_segementations.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "689ca792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d|u|r|m*i*ó*</td>\n",
       "      <td>d|u|r|m|i*ó*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g|u|s|t*a*</td>\n",
       "      <td>g|u|s|t|a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g|r|i|t*o*s*</td>\n",
       "      <td>g|r|i|t*o|s*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m|a|t*ó*</td>\n",
       "      <td>m|a|t*ó*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m|u|n|i|c|i|p*a|l*</td>\n",
       "      <td>m|u*n|i*c|i*p|a*l*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>o|j|a|l|á*</td>\n",
       "      <td>o|j*a*l|á*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "      <td>n|o|c|h|e*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f|l|a|c*o*</td>\n",
       "      <td>f|l|a|c|o*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m*í*a*</td>\n",
       "      <td>m|í*a*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s|i|g*a*</td>\n",
       "      <td>s|i*g|a*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 gold          prediction\n",
       "0        d|u|r|m*i*ó*        d|u|r|m|i*ó*\n",
       "1          g|u|s|t*a*          g|u|s|t|a*\n",
       "2        g|r|i|t*o*s*        g|r|i|t*o|s*\n",
       "3            m|a|t*ó*            m|a|t*ó*\n",
       "4  m|u|n|i|c|i|p*a|l*  m|u*n|i*c|i*p|a*l*\n",
       "5          o|j|a|l|á*          o|j*a*l|á*\n",
       "6          n|o|c|h|e*          n|o|c|h|e*\n",
       "7          f|l|a|c*o*          f|l|a|c|o*\n",
       "8              m*í*a*              m|í*a*\n",
       "9            s|i|g*a*            s|i*g|a*"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulm_segementations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365d191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
